{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ba9cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba6ba11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_season_schedule(db, year):\n",
    "    \"\"\"\n",
    "    Fetch the season schedule for a given year from the database.\n",
    "    \"\"\"\n",
    "    schedule_df = db.sql(\n",
    "        f\"\"\"\n",
    "        SELECT\n",
    "            Year, Week, Home_Team, Away_Team,\n",
    "            Is_Neutral, Home_days_Since_Last_Game, Away_days_Since_Last_Game\n",
    "        FROM game_features\n",
    "        WHERE Year = {year}\n",
    "        ORDER BY Week, Home_Team, Away_Team\n",
    "        \"\"\"\n",
    "    ).df()\n",
    "    return schedule_df\n",
    "\n",
    "def get_season_week_speads(db, year, week):\n",
    "    \"\"\"\n",
    "    Fetch the season spreads for a given year and week from the database.\n",
    "    \"\"\"\n",
    "    spread_df = db.sql(\n",
    "        f\"\"\"\n",
    "        SELECT\n",
    "            Home_Team, Away_Team, Spread\n",
    "        FROM game_features\n",
    "        WHERE Year = {year} AND Week = {week}\n",
    "        ORDER BY Home_Team, Away_Team\n",
    "        \"\"\"\n",
    "    ).df()\n",
    "    return spread_df\n",
    "\n",
    "def get_season_week_rankings(db, year, week):\n",
    "    \"\"\"\n",
    "    Fetch the season rankings for a given year and week from the database.\n",
    "    \"\"\"\n",
    "    rank_df = db.sql(\n",
    "        f\"\"\"\n",
    "        SELECT\n",
    "            Team,\n",
    "            ROW_NUMBER() OVER (PARTITION BY Year, Week ORDER BY Rating DESC) AS Rank\n",
    "        FROM nfl_rankings\n",
    "        WHERE Year = {year} AND Week = {week}\n",
    "        ORDER BY Team\n",
    "        \"\"\"\n",
    "    ).df()\n",
    "    return rank_df\n",
    "\n",
    "def get_team_records_from_db(db, year, week):\n",
    "    \"\"\"\n",
    "    Generate team_records dict for all teams up to (but not including) the given week.\n",
    "    \"\"\"\n",
    "    query = f\"\"\"\n",
    "        SELECT Home_Team, Away_Team, Home_Won\n",
    "        FROM game_features\n",
    "        WHERE Year = {year} AND Week < {week}\n",
    "    \"\"\"\n",
    "    df = db.sql(query).df()\n",
    "    team_records = {}\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        home = row[\"Home_Team\"]\n",
    "        away = row[\"Away_Team\"]\n",
    "        home_won = row[\"Home_Won\"]\n",
    "\n",
    "        for team in [home, away]:\n",
    "            if team not in team_records:\n",
    "                team_records[team] = {\"wins\": 0, \"losses\": 0, \"games_played\": 0}\n",
    "\n",
    "        # Update games played\n",
    "        team_records[home][\"games_played\"] += 1\n",
    "        team_records[away][\"games_played\"] += 1\n",
    "\n",
    "        # Update wins/losses\n",
    "        if home_won:\n",
    "            team_records[home][\"wins\"] += 1\n",
    "            team_records[away][\"losses\"] += 1\n",
    "        else:\n",
    "            team_records[away][\"wins\"] += 1\n",
    "            team_records[home][\"losses\"] += 1\n",
    "\n",
    "    return team_records\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b54b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudpickle as pickle\n",
    "\n",
    "with open('./models/lr_full.pkl', 'rb') as f:\n",
    "    full_model = pickle.load(f)\n",
    "with open('./models/lr_no_spread.pkl', 'rb') as f:\n",
    "    no_spread_model = pickle.load(f)\n",
    "\n",
    "models = {\n",
    "    'full': full_model,\n",
    "    'no_spread': no_spread_model\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392a36d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_results(path):\n",
    "    # Read the new CSV file\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    # Extract first picks and their log probabilities\n",
    "    first_steps = list(zip(df['week_1'], df['log_prob']))\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df_first = pd.DataFrame(first_steps, columns=['First_Team', 'Log_Prob'])\n",
    "\n",
    "    # Convert log probabilities to probabilities\n",
    "    df_first['Prob'] = np.exp(df_first['Log_Prob'])\n",
    "\n",
    "    # Aggregate: mean and std of probabilities for each first step\n",
    "    r = df_first.groupby('First_Team')['Prob'].agg(['mean', 'std', 'count', 'sum']).reset_index()\n",
    "    r.rename(columns={'mean': 'Avg_Prob', 'std': 'Std_Prob', 'count': 'Paths', 'sum': 'Sum_Prob'}, inplace=True)\n",
    "    r.sort_values(by=[\"Sum_Prob\"], ascending=False, inplace=True)\n",
    "\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778ca9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simulation.season import BeamExploreSeason\n",
    "\n",
    "def run_greedy_beam_path(year, models, schedule_df, k=1000):\n",
    "    survivor_picks = []\n",
    "    prior_weeks = {}\n",
    "    path = []\n",
    "    max_week = schedule_df['Week'].max()\n",
    "    for wk in range(1, max_week + 1):\n",
    "        with duckdb.connect('./data/data.db') as db:\n",
    "            spread_df = get_season_week_speads(db, year, wk)\n",
    "            rank_df = get_season_week_rankings(db, year, wk)\n",
    "            prior_weeks = get_team_records_from_db(db, year, wk)\n",
    "\n",
    "        beams = BeamExploreSeason(year, models, schedule_df, schedule_df.copy())\n",
    "        # Use real prior_weeks if you have it, otherwise pass as is\n",
    "        bp = beams.resolve(\n",
    "            week=wk, end_week=max_week,\n",
    "            spread=spread_df,\n",
    "            rank=rank_df,\n",
    "            k=k,\n",
    "            n=1,\n",
    "            survivor_picks=survivor_picks,\n",
    "            prior_weeks=prior_weeks\n",
    "        )\n",
    "\n",
    "        # Aggregate log_prob for each possible pick\n",
    "        pick_scores = {}\n",
    "        for path_obj in bp:\n",
    "            pick = path_obj['picks'][wk - 1]  # Get the pick for the current week\n",
    "            pick_scores.setdefault(pick, 0)\n",
    "            pick_scores[pick] += np.exp(path_obj['p'])\n",
    "\n",
    "        # Select the team with the highest sum log_prob\n",
    "        best_pick = max(pick_scores, key=pick_scores.get)\n",
    "        path.append(best_pick)\n",
    "        survivor_picks = path.copy()\n",
    "        # print (f\"Survivor picks so far: {survivor_picks}\")\n",
    "\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3359d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# year = 2024\n",
    "\n",
    "# with duckdb.connect('./data/data.db') as db:\n",
    "#     schedule_df = get_season_schedule(db, year)\n",
    "\n",
    "# greedy_path = run_greedy_beam_path(\n",
    "#     year, models, schedule_df, k=10000\n",
    "# )\n",
    "# print(\"Best greedy path:\", greedy_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a09de25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "for year in range(2024, 2012, -1):\n",
    "    print(f\"Running greedy path for year: {year}\")\n",
    "\n",
    "    with duckdb.connect('./data/data.db') as db:\n",
    "        schedule_df = get_season_schedule(db, year)\n",
    "\n",
    "    greedy_path = run_greedy_beam_path(\n",
    "        year, models, schedule_df, k=10_000\n",
    "    )\n",
    "    print(\"Best greedy path:\", greedy_path)\n",
    "    with open(f'./results/greedy_path_{year}.json', 'wb') as f:\n",
    "        f.write(json.dumps(greedy_path).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34887cc5",
   "metadata": {},
   "source": [
    "Best greedy path: ['Seattle', 'Baltimore', 'Cleveland', 'San Francisco', 'Miami', 'Atlanta', 'Washington', 'Denver', 'Cincinnati', 'LA Chargers', 'Detroit', 'Houston', 'Kansas City', 'Philadelphia', 'Minnesota', 'Green Bay', 'Tampa Bay', 'Arizona']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89c970f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
